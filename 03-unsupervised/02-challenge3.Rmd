---
title: "Challenge 3"
author: Ryan Wesslen
date: July 27, 2016
output: html_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=TRUE, warning=FALSE, message=FALSE)
```

#### Unsupervised machine learning

We're going to run topic modeling on Tweets - the exception is we're going to combine all users Tweets, essentially running a user-level topic modeling.

1. First, read our Charlotte sample dataset and create the DFM object. Trim all words used less than 40 times.

```{r}

```

2. Now, estimate the topic model using a number of topics that you think is reasonable.

```{r}

```

3. Find the user with the highest topic probability in Topic 1. Extract the actor.id and then filter the original dataset to find all Tweets by the user.

How accurate was the topic to finding the "highest" user of the Tweets?

```{r}

```

4. Find a different topic. Find the top 5 users and similarly filter out their tweets. 

How accurate were the Tweets to the topic? What other topics does the actor have a high probability (> 10%)? Do the users' Tweets have some information related?

```{r}

```

BONUS. Experiment with different number of topics. Which value appears to be more appropriate?

BONUS. BONUS. Use `KL.empirical` from the `entropy` library to calculate KL Divergence by user's topic distributions. This will give a similarity measure (e.g., recommender) that will measure which users are most similar to this user. 

```{r}
```
