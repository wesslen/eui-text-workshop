---
title: "Challenge 3"
author: Ryan Wesslen
date: July 27, 2016
output: html_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=TRUE, warning=FALSE, message=FALSE)
```

#### Unsupervised machine learning

We're going to run topic modeling on Tweets - the exception is we're going to combine all users' Tweets by each user, essentially running a user-level topic modeling.

1. First, read our Charlotte sample dataset and create the DFM object. Trim all words used less than 40 times.

```{r}
tweets <- read.csv('../datasets/CharlotteTweets20Sample.csv', stringsAsFactors=F)
source('../functions.R')

library(quanteda)
twcorpus <- corpus(tweets$body)
docvars(twcorpus, "actor.id") <- as.character(tweets$actor.id) 
twdfm <- dfm(twcorpus, groups = "actor.id", ignoredFeatures = c(stopwords("english"), "t.co", "https", "rt", "amp", "http", "t.c", "can", "u"), removeTwitter = TRUE, ngrams=c(1,2))
twdfm <- trim(twdfm, minDoc = 40)
topfeatures(twdfm)
```

2. Now, estimate the topic model using a number of topics that you think is reasonable. 

```{r}
# install.packages("topicmodels")
library(topicmodels)

# we now export to a format that we can run the topic model with
dtm <- convert(twdfm, to="topicmodels")

# estimate LDA with K topics
K <- 20
lda <- LDA(dtm, k = K, method = "Gibbs", 
                control = list(verbose=25L, seed = 123, burnin = 100, iter = 500))

term <- terms(lda, 10)
colnames(term) <- paste("Topic",1:K)
term
```

If you want to run a LDA interactive visualization, run this chunk:

```{r, eval=FALSE, include=FALSE}
#Create Json for LDAVis
library(LDAvis)
json <- topicmodels_json_ldavis(lda,twdfm,dtm)
new.order <- RJSONIO::fromJSON(json)$topic.order

term <- terms(lda, 10)
term <- term[,new.order]
colnames(term) <- paste("Topic",1:K)
term

serVis(json, out.dir = 'charlotteLDA', open.browser = TRUE)
```

3. Find the user with the highest topic probability in Topic 1. Extract the actor.id and then filter the original dataset to find all Tweets by the user.

How accurate was the topic to finding the "highest" user of the Tweets?

```{r}
# to get topic probabilities per actor ID (Twitter user)
postlist <- posterior(lda)
probtopics <- data.frame(postlist$topics)
#probtopics <- probtopics[,new.order]
colnames(probtopics) <- paste("Topic",1:K)

row <- order(-probtopics$`Topic 1`)
actorid <- rownames(probtopics[row[1],])
filter.data <- subset(tweets, actor.id == actorid)
```

4. Find a different topic. Find the top 5 users and similarly filter out their tweets. 

How accurate were the Tweets to the topic? What other topics does the actor have a high probability (> 10%)? Do the users' Tweets have some information related?

```{r}

```

BONUS. Experiment with different number of topics. Which value appears to be more appropriate?

```{r}
# install.packages("cvTools")
require(cvTools)

K <- c(5, 10, 20, 30, 40)

results <- list()

i = 1
for (k in K){
    cat("\n\n\n##########\n ", k, "topics", "\n")
    res <- cvLDA(k, dtm)
    results[[i]] <- res
    i = i + 1
}

topicPlots(results,K)
```

BONUS. BONUS. Use `KL.empirical` from the `entropy` library to calculate KL Divergence by user's topic distributions. This will give a similarity measure (e.g., recommender) that will measure which users are most similar to this user. 
